{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Flair Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this project, I have used the Python Reddit API wrapper (PRAW) to scrape data from the Reddit India subreddit. This is a summary of my work to pre-process, train and classify the data obtained for each post:**\n",
    "1. **Pre-processing for labels and date-time variables**\n",
    "2. **Natural Language Processing for text data**\n",
    "3. **Test-train split of data to check various classifiers**\n",
    "4. **Comparing various classifiers and their parameters to get the best accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.executable\n",
    "! pip install praw\n",
    "! pip install plotly\n",
    "! pip install --upgrade sklearn\n",
    "# !{sys.executable} -m pip install praw\n",
    "# ! conda install -n curr_env scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary Python libraries \n",
    "import praw\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.graph_objs as po\n",
    "import numpy as np\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Reddit instance and a dictionary to store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " reddit=praw.Reddit(client_id=\"xaMlfF_8S0xOlw\",client_secret=\"Hk1K5nCd5v7Hvg9TRkMZo-CELYE\",user_agent=\"R_WebScraping\")\n",
    "\n",
    "# Dictionary to store post data \n",
    "reddict = {'title': [], 'selftext': [], 'time' : [], 'upvote_ratio':[],'flair':[]}\n",
    "posts=reddit.subreddit('india').hot(limit=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the neccessary information for each post\n",
    "\n",
    "* **selftext : The body text of the post**\n",
    "* **title : The title of the post**\n",
    "* **id : The id assigned to each post**\n",
    "* **num_comments : The number of comments for the post**\n",
    "* **score : The number of upvotes for the post**\n",
    "* **upvote_ratio : The percentage of upvotes against all votes for the post**\n",
    "* **author : The author of the post**\n",
    "* **comments : The comment text for each post**\n",
    "* **time : The UTC time of creation of the post**\n",
    "* **flair : The Flair assigned to the post - This is our target variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time(submission):\n",
    "    time=submission.created\n",
    "    return time\n",
    "\n",
    "for post in posts:\n",
    "    reddict['selftext'].append(post.selftext)\n",
    "    reddict['title'].append(post.title)\n",
    "#     reddict['id'].append(post.id)\n",
    "#     reddict['num_comments'].append(post.num_comments)\n",
    "#     reddict['score'].append(post.score)\n",
    "#     reddict['ups'].append(post.ups)\n",
    "#     reddict['downs'].append(post.downs)\n",
    "    reddict['upvote_ratio'].append(post.upvote_ratio)\n",
    "#     reddict['author'].append(post.author)\n",
    "#     reddict['comments'].append(post.comments)\n",
    "    reddict['time'].append(get_time(post))\n",
    "    reddict['flair'].append(post.link_flair_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the data obtained in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.DataFrame(reddict)\n",
    "\n",
    "csv = f'indiaReddit.csv'\n",
    "with open(csv,'w',newline='',encoding='utf-8') as f_out:\n",
    "    reddit_df.to_csv(csv, index = False, header=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from praw.models import MoreComments\n",
    "\n",
    "# def comments(text):\n",
    "#     comments_list = []\n",
    "#     for top_level_comment in text:\n",
    "#         if isinstance(top_level_comment, MoreComments):\n",
    "#             continue\n",
    "#         comments_list.append(top_level_comment.body)\n",
    "#     return comments_list\n",
    "    \n",
    "# reddit_df[\"comments_body\"] = reddit_df[\"comments\"].apply(lambda x: comments(x))\n",
    "# print(reddit_df[\"comments_body\"][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some exploratory data analysis\n",
    "\n",
    "* **A distribution of the unique flairs obtained from the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.describe of                                                  title  \\\n",
      "0    Coronavirus (COVID-19) Megathread - News and U...   \n",
      "1    Announcing r/IndiaMeme, our own sub for memes ...   \n",
      "2                         1937 advertisement for India   \n",
      "3    Indore: Porsche Driver Sit-Ups As Punishment F...   \n",
      "4    Covid-19 cess, 40% tax for rich - IRS officers...   \n",
      "5    India's National Board for Wildlife Is a Big T...   \n",
      "6    Arnab Goswami tells SC all parties in Palghar ...   \n",
      "7    India can’t conquer COVID-19 without aggressiv...   \n",
      "8    Jharkhand Police swiftly act to book communal ...   \n",
      "9                                        PM Cares Fund   \n",
      "10   My friend went to a barber 10 days back, barbe...   \n",
      "11   Reliance hired lobbyist Brian Ballard, a close...   \n",
      "12   How the Indian Railways is reinventing itself ...   \n",
      "13   How come shops are allowed to sell non-essenti...   \n",
      "14        Experimenting things. Rainy day at my place.   \n",
      "15   24 in Vijayawada contract virus as man hosts g...   \n",
      "16   Colours of India | Assam [Xiaomi POCO F1 / 192...   \n",
      "17   Forget Hindu-Muslim, donate plasma, save lives...   \n",
      "18   'Clinically depressed' woman smothers 11-month...   \n",
      "19   Govt Discomfort With Rel Jio Tower Deal Over F...   \n",
      "20   Online cheat dupes techie of Rs 30,000 after p...   \n",
      "21   Why Sitharaman’s 'Pulses For All' Promise Stil...   \n",
      "22         Recovered, Tablighis ready to donate plasma   \n",
      "23              \"Doland, Don't do Press Conference...\"   \n",
      "24   In Gujarat, Muslims counter hate with police c...   \n",
      "25   Coronavirus | No new COVID-19 cases after May ...   \n",
      "26   BR Shetty: The staggering rise and incredible ...   \n",
      "27   Delhi ‘forced to agree’, Maharashtra says no —...   \n",
      "28   How Kerala is feeding its 3.48 crore residents...   \n",
      "29   How smokers are getting by without cigarettes ...   \n",
      "..                                                 ...   \n",
      "736              Mile Sur Mera Tumhara (Good old days)   \n",
      "737  Xiaomi Redmi Pad 5G tablet to get 90Hz display...   \n",
      "738  Coronavirus epidemic could peak in India by mi...   \n",
      "739                                        Kota crisis   \n",
      "740          Four militants shot dead in J&K’s Shopian   \n",
      "741  People of r/india, how is your Work from Home ...   \n",
      "742  Health Ministry wants export of anti-TB drug p...   \n",
      "743  Mumbai Hospital Issues Notice To Docs On Strik...   \n",
      "744              I'm really in deep trouble right now.   \n",
      "745  Can someone help me surprise my girlfriend? (N...   \n",
      "746  By crowdfunding benefits for embattled workers...   \n",
      "747  Indian Medical Association (IMA) withdraws the...   \n",
      "748  UAE princess slams rampant racism and Islamoph...   \n",
      "749  Can I bring a vibrator/sex toy from USA to Ind...   \n",
      "750  Delhi: Union HM Amit Shah & Union Health Minis...   \n",
      "751  Any alternatives to inshorts? Or any better ne...   \n",
      "752  Senior Officers’ Statements Contradict FIRs on...   \n",
      "753  Opinion | In India, a Pandemic of Prejudice an...   \n",
      "754  Those who attack Arnab Goswami are enemies of ...   \n",
      "755  12-Year-Old Walks 3 Days Amid Lockdown, Dies J...   \n",
      "756  Why Is the Centre Pushing a New Electricity Bi...   \n",
      "757  Senior J&K Cop Called Out For Tweet Against PM...   \n",
      "758  The Hindu journalist questioned by J&K police ...   \n",
      "759  COVID-19: Doctors in Aligarh buy PPE worth Rs ...   \n",
      "760  26/11 Mumbai attack mastermind Zaki-ur-Rehman ...   \n",
      "761                Hunger devours shame in food queues   \n",
      "762  very sad that #Islamophobia_In_India trends no...   \n",
      "763  Jamaat attendees in Tamil Nadu who recovered f...   \n",
      "764  National Human Rights Commission issues notice...   \n",
      "765  Now that the world has spotted Hindutva bigotr...   \n",
      "\n",
      "                                              selftext          time  \\\n",
      "0    ###[Covid-19 Fundraisers & Donation Links](htt...  1.587010e+09   \n",
      "1    HELLO YOU NICE PEOPLE. WE GOT REALLY TIRED OF ...  1.586716e+09   \n",
      "2                                                       1.587908e+09   \n",
      "3                                                       1.587904e+09   \n",
      "4                                                       1.587904e+09   \n",
      "5                                                       1.587908e+09   \n",
      "6    Show the bhakts this next time they shed croco...  1.587903e+09   \n",
      "7                                                       1.587920e+09   \n",
      "8                                                       1.587886e+09   \n",
      "9                                                       1.587858e+09   \n",
      "10   My friend went to a barber 10 days back, barbe...  1.587908e+09   \n",
      "11                                                      1.587909e+09   \n",
      "12                                                      1.587897e+09   \n",
      "13   This is quite apparent from yesterday's circul...  1.587904e+09   \n",
      "14                                                      1.587902e+09   \n",
      "15                                                      1.587911e+09   \n",
      "16                                                      1.587925e+09   \n",
      "17                                                      1.587924e+09   \n",
      "18                                                      1.587913e+09   \n",
      "19                                                      1.587908e+09   \n",
      "20                                                      1.587895e+09   \n",
      "21                                                      1.587909e+09   \n",
      "22                                                      1.587915e+09   \n",
      "23                                                      1.587834e+09   \n",
      "24                                                      1.587923e+09   \n",
      "25                                                      1.587904e+09   \n",
      "26                                                      1.587908e+09   \n",
      "27                                                      1.587895e+09   \n",
      "28                                                      1.587916e+09   \n",
      "29                                                      1.587914e+09   \n",
      "..                                                 ...           ...   \n",
      "736                                                     1.587620e+09   \n",
      "737                                                     1.587620e+09   \n",
      "738                                                     1.587561e+09   \n",
      "739  Someone I know is stuck in Kota without having...  1.587590e+09   \n",
      "740                                                     1.587568e+09   \n",
      "741  Just interested in how WFH is actually coming ...  1.587552e+09   \n",
      "742                                                     1.587596e+09   \n",
      "743                                                     1.587568e+09   \n",
      "744  My laptop's charging port broke and now I have...  1.587600e+09   \n",
      "745  So my girlfriend is Indian and she and I live ...  1.587588e+09   \n",
      "746                                                     1.587562e+09   \n",
      "747                                                     1.587566e+09   \n",
      "748                                                     1.587559e+09   \n",
      "749  I am an Indian citizen and plan on traveling t...  1.587568e+09   \n",
      "750                                                     1.587561e+09   \n",
      "751  Too much fillers and useless stuffs shown ther...  1.587586e+09   \n",
      "752                                                     1.587583e+09   \n",
      "753                                                     1.587582e+09   \n",
      "754                                                     1.587654e+09   \n",
      "755                                                     1.587475e+09   \n",
      "756                                                     1.587562e+09   \n",
      "757                                                     1.587562e+09   \n",
      "758                                                     1.587563e+09   \n",
      "759                                                     1.587566e+09   \n",
      "760                                                     1.587500e+09   \n",
      "761                                                     1.587574e+09   \n",
      "762   The covid pandemic and subseqent portrayal of...  1.587514e+09   \n",
      "763                                                     1.587503e+09   \n",
      "764                                                     1.587543e+09   \n",
      "765                                                     1.587509e+09   \n",
      "\n",
      "     upvote_ratio               flair  \n",
      "0            0.96         Coronavirus  \n",
      "1            0.89        Announcement  \n",
      "2            0.98       Non-Political  \n",
      "3            0.95       Non-Political  \n",
      "4            0.96         Coronavirus  \n",
      "5            0.93  Science/Technology  \n",
      "6            0.89            Politics  \n",
      "7            0.90         Coronavirus  \n",
      "8            0.89         Coronavirus  \n",
      "9            0.89            Politics  \n",
      "10           0.93         Coronavirus  \n",
      "11           0.96            Politics  \n",
      "12           0.96       Non-Political  \n",
      "13           0.88            AskIndia  \n",
      "14           0.92         Photography  \n",
      "15           0.96         Coronavirus  \n",
      "16           0.96         Photography  \n",
      "17           0.90         Coronavirus  \n",
      "18           0.97       Non-Political  \n",
      "19           0.95    Business/Finance  \n",
      "20           0.97         Coronavirus  \n",
      "21           0.88      Policy/Economy  \n",
      "22           0.79         Coronavirus  \n",
      "23           0.94            Politics  \n",
      "24           0.93       Non-Political  \n",
      "25           0.87         Coronavirus  \n",
      "26           0.95    Business/Finance  \n",
      "27           0.96         Coronavirus  \n",
      "28           0.80      Policy/Economy  \n",
      "29           0.87         Coronavirus  \n",
      "..            ...                 ...  \n",
      "736          0.56       Non-Political  \n",
      "737          0.50  Science/Technology  \n",
      "738          0.90         Coronavirus  \n",
      "739          0.90            AskIndia  \n",
      "740          0.83       Non-Political  \n",
      "741          0.90            AskIndia  \n",
      "742          0.77      Policy/Economy  \n",
      "743          0.92         Coronavirus  \n",
      "744          0.72       Non-Political  \n",
      "745          0.72            AskIndia  \n",
      "746          0.94    Business/Finance  \n",
      "747          0.93         Coronavirus  \n",
      "748          0.68            Politics  \n",
      "749          0.84            AskIndia  \n",
      "750          0.83         Coronavirus  \n",
      "751          0.82            AskIndia  \n",
      "752          0.67            Politics  \n",
      "753          0.64            Politics  \n",
      "754          0.45            Politics  \n",
      "755          0.96         Coronavirus  \n",
      "756          0.76      Policy/Economy  \n",
      "757          0.77            Politics  \n",
      "758          0.76            Politics  \n",
      "759          0.76         Coronavirus  \n",
      "760          0.96            Politics  \n",
      "761          0.81      Policy/Economy  \n",
      "762          0.81            Politics  \n",
      "763          0.87         Coronavirus  \n",
      "764          0.95            Politics  \n",
      "765          0.77         Coronavirus  \n",
      "\n",
      "[766 rows x 5 columns]>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f271fe0c362d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreddit_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0munique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreddit_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'flair'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid axis kwarg specified for unique'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# reddit_df = pd.read_csv(\"indiaReddit.csv\")  \n",
    "\n",
    "#Basic data informaton\n",
    "print(reddit_df.describe)\n",
    "\n",
    "unique, counts = np.unique(list(reddit_df['flair']), return_counts=True)\n",
    "x=unique\n",
    "y=counts\n",
    "plt.bar(x,y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the DateTime object to get the day of week, month, year of the post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reddit_df['dateTime'] = pd.to_datetime(reddit_df['time'], \n",
    "#  format = '%Y-%m-%dT%H:%M:%SZ', \n",
    "#  errors = 'coerce')\n",
    "\n",
    "def get_time(submission):\n",
    "    time=submission.created\n",
    "    return datetime.datetime.fromtimestamp(time).strftime(\"%H\")\n",
    "reddit_df['month'] = reddit_df['time'].apply(lambda x: datetime.datetime.fromtimestamp(x).strftime(\"%m\"))\n",
    "reddit_df['month'] = reddit_df['month'].astype(int)\n",
    "reddit_df['hour'] = reddit_df['time'].apply(lambda x: datetime.datetime.fromtimestamp(x).strftime(\"%H\"))\n",
    "reddit_df['hour'] = reddit_df['hour'].astype(int)\n",
    "reddit_df['year'] = reddit_df['time'].apply(lambda x: datetime.datetime.fromtimestamp(x).strftime(\"%Y\"))\n",
    "reddit_df['year'] = reddit_df['year'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Natural Language Processing for the text data after combining selftext and title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aditikulkarni/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aditikulkarni/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "['india', 'covid19', 'lockdown', 'time', 'indian', 'people', 'like', 'would', 'government', 'get', 'news', 'one', 'also', 'day', 'help', 'go', 'know', 'need', 'dont', 'think', 'say', 'country', 'month', 'im', 'since', 'good', 'back', 'year', 'delhi', 'could', 'still', 'health', 'guy', 'police', 'problem', 'service', 'much', 'part', 'caste', 'pandemic', 'end', 'last', 'school', 'anyone', 'better', 'open', 'he', 'girl', 'business', 'high', 'ncrb', 'sure', 'china', 'trai', 'neutrality', 'motivational', 'card', 'bangladesh', 'rfp', 'liberation', 'strong']\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation\n",
    "\n",
    "string.punctuation = string.punctuation+''\n",
    "def noPunc(text):\n",
    "    textNoPunc = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return textNoPunc\n",
    "\n",
    "reddit_df[\"title_selftext\"] = reddit_df[\"title\"].astype(str) + reddit_df[\"selftext\"].astype(str)\n",
    "reddit_df[\"title_selftext_noPunc\"] = reddit_df[\"title_selftext\"].apply(lambda x: noPunc(x))\n",
    "\n",
    "\n",
    "#Tokenize using regex\n",
    "import re\n",
    "\n",
    "reddit_df[\"title_selftext_tokenized2\"] = reddit_df[\"title_selftext_noPunc\"].apply(lambda x: re.split('\\W+', x.lower()))\n",
    "\n",
    "#Remove stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def noStopWords(text):\n",
    "    textNoStops = [word.lower() for word in text if word not in nltk.corpus.stopwords.words('english')]\n",
    "    return textNoStops\n",
    "\n",
    "reddit_df[\"title_selftext_noStops\"] = reddit_df[\"title_selftext_tokenized2\"].apply(lambda x : noStopWords(x))\n",
    "\n",
    "#Lemmatizer instead of stemmer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wn = WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    lemmatized = [wn.lemmatize(word) for word in text]\n",
    "    return lemmatized\n",
    "\n",
    "reddit_df[\"title_selftext_lemmatized\"] = reddit_df[\"title_selftext_noStops\"].apply(lambda x: lemmatizer(x))\n",
    "\n",
    "\n",
    "oneList = []\n",
    "corpusList = []\n",
    "def corpusCreate(text):\n",
    "    oneList.append(text)\n",
    "    for word in text:\n",
    "        corpusList.append(word)\n",
    "       \n",
    "def createMappingMatrix(corpus, list_check, dictCorpus, lenCorpus):\n",
    "    count = 0\n",
    "    sampleData = [0]*lenCorpus\n",
    "    for word in list_check:\n",
    "        if word in corpus:\n",
    "            sampleData[dictCorpus[word]] = 1\n",
    "        return sampleData    \n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "reddit_df[\"title_selftext_lemmatized\"].apply(lambda x: corpusCreate(x))\n",
    "countOfWords = dict(Counter(corpusList))\n",
    "countOfWords = {v: k for k, v in countOfWords.items()}\n",
    "revc = list(sorted(countOfWords.keys(), reverse = True))\n",
    "corpus_sorted = [countOfWords[i] for i in revc if i>5]\n",
    "dict_corpus_sorted = {word:i for i,word in enumerate(corpus_sorted)}\n",
    "# print(dict_corpus_sorted[\"video\"])\n",
    "print(corpus_sorted)\n",
    "\n",
    "modelData = []\n",
    "for i in range(len(reddit_df)):\n",
    "    modelData.append(createMappingMatrix(corpus_sorted, reddit_df.loc[i,\"title_selftext_lemmatized\"] , dict_corpus_sorted, len(corpus_sorted)))\n",
    "print(len(corpus_sorted[0]))\n",
    "\n",
    "modeldf = pd.DataFrame(modelData, columns = [term for term in corpus_sorted])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing NaN and null values from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reddit_df['ups'] = reddit_df['ups'].replace(np.nan, 0, regex=True)\n",
    "# modeldf['ups'] = reddit_df['ups']\n",
    "# reddit_df['downs'] = reddit_df['downs'].replace(np.nan, 0, regex=True)\n",
    "# modeldf['downs'] = reddit_df['downs']\n",
    "# reddit_df['num_comments'] = reddit_df['num_comments'].replace(np.nan, 0, regex=True)\n",
    "# modeldf['num_comments'] = reddit_df['num_comments']\n",
    "# reddit_df['score'] = reddit_df['score'].replace(np.nan, 0, regex=True)\n",
    "# modeldf['score'] = reddit_df['score']\n",
    "# reddit_df['upvote_ratio'] = reddit_df['upvote_ratio'].replace(np.nan, 0, regex=True)\n",
    "# modeldf['upvote_ratio'] = reddit_df['upvote_ratio']\n",
    "modeldf['month'] = reddit_df['month'] \n",
    "\n",
    "modeldf['hour'] = reddit_df['hour']\n",
    "modeldf['year'] = reddit_df['year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LabelEncoder for the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = LabelEncoder()\n",
    "reddit_df['flair'] = reddit_df['flair'].replace(np.nan, 'NaN', regex=True)\n",
    "reddit_df['flair_vec'] = encoder.fit_transform(reddit_df['flair'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Train Splt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(modeldf,reddit_df['flair_vec'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning:\n",
      "\n",
      "The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "param = {'n_estimators' : [10, 150, 300,700, 1000, 2000], 'max_depth' : [30, 60, 90, None], \n",
    "         'min_samples_split' : [2, 5, 10], 'min_samples_leaf' : [1, 2, 4], 'bootstrap' : [True, False]}\n",
    "model_rf = GridSearchCV(rf, param, n_jobs = -1)\n",
    "\n",
    "# model_rf.fit(train_x,train_y)\n",
    "\n",
    "scores = cross_val_score(model_rf, train_x, train_y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean(), scores.std() * 2)\n",
    "# predict the target on the train dataset\n",
    "predict_train_rf = model_rf.predict(train_x)\n",
    "\n",
    "# Accuray Score on train dataset\n",
    "accuracy_train_rf = accuracy_score(train_y,predict_train_rf)\n",
    "print('accuracy_score on train dataset RandomForestClassifier: ', accuracy_train_rf)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test_rf = model_rf.predict(test_x)\n",
    "\n",
    "# Accuray Score on test dataset\n",
    "accuracy_test_rf = accuracy_score(test_y,predict_test_rf)\n",
    "print('accuracy_score on test dataset RandomForestClassifier: ', accuracy_test_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score on train dataset KNNeighbors:  0.3812636165577342\n",
      "accuracy_score on test dataset KNNeighbors:  0.3322475570032573\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# model_mnb = MultinomialNB()\n",
    "# predict_train = model_mnb.fit(train_x, train_y)\n",
    "\n",
    "# # predict the target on the train dataset\n",
    "# predict_train = model_mnb.predict(train_x)\n",
    "\n",
    "# # Accuray Score on train dataset\n",
    "# accuracy_train = accuracy_score(train_y,predict_train)\n",
    "# print('accuracy_score on train dataset KNNeighbors: ', accuracy_train)\n",
    "\n",
    "# # predict the target on the test dataset\n",
    "# predict_test = model_mnb.predict(test_x)\n",
    "\n",
    "# # Accuray Score on test dataset\n",
    "# accuracy_test = accuracy_score(test_y,predict_test)\n",
    "# print('accuracy_score on test dataset KNNeighbors: ', accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
